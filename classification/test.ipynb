{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "from model import Net\n",
    "from train import get_args, train\n",
    "from dataset import CLSDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python train.py --batch-size 1 --embedding-dim 128 --hidden-dim 128 --num-layers 2\n",
    "args = get_args(args=[\"--batch\", \"1\", \"--embedding-dim\", \"128\", \"--hidden-dim\", \"128\", \"--num-layers\", \"6\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CLSDataset()\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size, collate_fn=train_set.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"vocab_size\": 1264,\n",
      "    \"max_position_embeddings\": 6000,\n",
      "    \"n_embed\": 128,\n",
      "    \"n_layer\": 6,\n",
      "    \"n_head\": 8,\n",
      "    \"ffn_dim\": 128,\n",
      "    \"static_position_embeddings\": true,\n",
      "    \"pad_token_id\": 1\n",
      "}\n",
      "{\n",
      "    \"vocab_size\": 1264,\n",
      "    \"max_position_embeddings\": 800,\n",
      "    \"n_embed\": 128,\n",
      "    \"n_layer\": 6,\n",
      "    \"n_head\": 8,\n",
      "    \"ffn_dim\": 128,\n",
      "    \"static_position_embeddings\": true,\n",
      "    \"pad_token_id\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model = Net(args, train_set.dictionary).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11M\tmodel.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"model.pt\")\n",
    "# check size of model\n",
    "!du -sh model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the memory size of the model\n",
    "\n",
    "inputs = next(iter(train_loader))\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_bytes2human_readable(num_bytes):\n",
    "    flag = False\n",
    "    if num_bytes < 0:\n",
    "        flag = True\n",
    "        num_bytes = -num_bytes\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num_bytes < 1024.0:\n",
    "            return \"%3.1f %s\" % (num_bytes, x) if not flag else \"-%3.1f %s\" % (num_bytes, x)\n",
    "        num_bytes /= 1024.0\n",
    "    return \"%3.1f %s\" % (num_bytes, 'TB') if not flag else \"-%3.1f %s\" % (num_bytes, 'TB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_memory(model, inputs):\n",
    "    model.cpu()\n",
    "    inputs = {k: v.cpu() for k, v in inputs.items()}\n",
    "    torch.cuda.empty_cache()\n",
    "    before_mem = torch.cuda.memory_allocated(device)\n",
    "\n",
    "    model.to(device)\n",
    "    model_memory = torch.cuda.memory_allocated(device) - before_mem\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    inputs_memory = torch.cuda.memory_allocated(device) - before_mem - model_memory\n",
    "\n",
    "    loss = model.get_loss(**inputs)\n",
    "    compute_memory = torch.cuda.memory_allocated(device) - before_mem - model_memory - inputs_memory\n",
    "\n",
    "    loss.backward()\n",
    "    back_prop_memory = torch.cuda.memory_allocated(device) - before_mem - model_memory - inputs_memory - compute_memory\n",
    "\n",
    "    # inputs_memory = sum([v.element_size() * v.nelement() for v in inputs.values()])\n",
    "    return(\n",
    "        num_bytes2human_readable(model_memory),\n",
    "        num_bytes2human_readable(inputs_memory),\n",
    "        num_bytes2human_readable(compute_memory),\n",
    "        num_bytes2human_readable(back_prop_memory),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('10.1 MB', '8.0 KB', '168.4 MB', '-161.7 MB')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_memory(model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "for samples in train_loader:\n",
    "    choices = samples[\"choices\"]\n",
    "    max_length = max(max_length, choices.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c569748d09bf995aa9188fee71db78eaf3c1ca75f9d5c35924394343f8c3975"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
