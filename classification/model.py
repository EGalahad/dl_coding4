import copy
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

import json

from transformer import TransformerDecoder, TransformerEncoder, Config, make_padding_mask

class Net(nn.Module):

    def __init__(self, args, dictionary):
        super().__init__()
        ##############################################################################
        #                  TODO: You need to complete the code here                  #
        ##############################################################################
        self.dictionary = dictionary
        self.vocab_size = len(dictionary)
        self.padding_idx = dictionary.pad()
        self.q_idx = dictionary.indices["<q>"]
        self.cls_idx = dictionary.indices["<cls>"]

        self.embedding_dim = args.embedding_dim
        self.hidden_dim = args.hidden_dim
        self.num_layers = args.num_layers

        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=self.padding_idx)

        self.encoder_config = Config(
            vocab_size=self.vocab_size,
            max_position_embeddings=6000,
            n_embed=self.embedding_dim,
            n_layer=self.num_layers,
            n_head=4,
            ffn_dim=self.hidden_dim,
            static_position_embeddings=True,
            pad_token_id=self.padding_idx,
        )

        self.decoder_config = Config(
            vocab_size=self.vocab_size,
            max_position_embeddings=800,
            n_embed=self.embedding_dim,
            n_layer=self.num_layers,
            n_head=4,
            ffn_dim=self.hidden_dim,
            static_position_embeddings=True,
            pad_token_id=self.padding_idx,
        )

        print(json.dumps(self.encoder_config.__dict__, indent=4))
        print(json.dumps(self.decoder_config.__dict__, indent=4))

        self.encoder = TransformerEncoder(self.encoder_config, self.embedding)
        self.decoder = TransformerDecoder(self.decoder_config, self.embedding)

        self.classifier = nn.Sequential(
            nn.Linear(self.embedding_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Linear(self.hidden_dim, 1),
        )

        ##############################################################################
        #                              END OF YOUR CODE                              #
        ##############################################################################

    def logits(self, **kwargs):
        """
        Compute the logits for the input data.

        Args:
            kwargs (dict): Custom keyword arguments containing input data.
                          Modify the input processing according to your needs.

        Returns:
            logits (tensor): Logits generated by your model given input.
        """
        # ##############################################################################
        #                  TODO: You need to complete the code here                  #
        ##############################################################################
        content = kwargs["content"]
        q = kwargs["q"]
        choices = kwargs["choices"]
        batch_size = content.shape[0]

        invalid_choices_mask = (choices == self.padding_idx).all(dim=-1)
        # invalid_choices_mask: (batch_size, 4)

        # make the input to the encoder: content <q> question
        content_len = content.shape[1]
        q_len = q.shape[1]
        content_q = torch.cat([content, torch.full((batch_size, 1), self.q_idx, dtype=torch.long, device=content.device), q], dim=1)
        # content_q: (batch_size, content_len + q_len + 1)
        content_q_padding_mask = make_padding_mask(content_q, self.padding_idx)
        
        content_q_encoded = self.encoder(content_q, content_q_padding_mask)
        # content_q_encoded: (batch_size, content_len + q_len + 1, hidden_dim)

        del content_q

        # make the input to the decoder: <cls> choice1 <cls> choice2 <cls> choice3 <cls> choice4
        choices_len = choices.shape[2]
        cls_choices = torch.cat([
            torch.full((batch_size, 1), self.cls_idx, dtype=torch.long, device=choices.device),
            choices[:, 0, :],
            torch.full((batch_size, 1), self.cls_idx, dtype=torch.long, device=choices.device),
            choices[:, 1, :],
            torch.full((batch_size, 1), self.cls_idx, dtype=torch.long, device=choices.device),
            choices[:, 2, :],
            torch.full((batch_size, 1), self.cls_idx, dtype=torch.long, device=choices.device),
            choices[:, 3, :],
        ], dim=1)
        # cls_choices: (batch_size, 4 * choices_len + 4)
        cls_choices_padding_mask = make_padding_mask(cls_choices, self.padding_idx)
        if cls_choices.shape[1] > self.decoder_config.max_position_embeddings:
            print(cls_choices.shape[1])
            raise Exception("too long")

        cls_choices_encoded = self.decoder(
            cls_choices,
            encoder_hidden_states=content_q_encoded,
            encoder_padding_mask=content_q_padding_mask,
            decoder_padding_mask=cls_choices_padding_mask,
            decoder_causal_mask=None,
        )
        # cls_choices_encoded: (batch_size, 4 * choices_len + 4, hidden_dim)

        del cls_choices

        content_q_pooled = content_q_encoded[:, 0, :]
        # content_q_pooled: (batch_size, hidden_dim)
        cls_choices_pooled = cls_choices_encoded[:, 0::choices_len + 1, :]
        # cls_choices_pooled: (batch_size, 4, hidden_dim)

        del content_q_encoded, cls_choices_encoded

        logits = self.classifier(content_q_pooled.unsqueeze(1) * cls_choices_pooled).squeeze(-1)
        # logits: (batch_size, 4)
        logits.masked_fill_(invalid_choices_mask, -1e9)
        return logits
        ##############################################################################
        #                              END OF YOUR CODE                              #
        ##############################################################################

    def get_loss(self, **kwargs):
        """
        Compute the loss for the input data and target labels.

        Args:
            kwargs (dict): Custom keyword arguments containing input data and
                           target labels. Modify the input and target processing
                           according to your needs.

        Returns:
            loss (tensor): The loss for the input data and target labels.
        """
        # ##############################################################################
        #                  TODO: You need to complete the code here                  #
        ##############################################################################
        targets = kwargs.pop("targets")
        logits = self.logits(**kwargs)
        loss = F.cross_entropy(logits, targets)
        return loss
        ##############################################################################
        #                              END OF YOUR CODE                              #
        ##############################################################################
