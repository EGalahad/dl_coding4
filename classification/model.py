import copy
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):

    def __init__(self, args, dictionary):
        super().__init__()
        ##############################################################################
        #                  TODO: You need to complete the code here                  #
        ##############################################################################
        self.n_embed = args.embedding_dim
        self.n_hidden = args.hidden_dim
        self.n_layers = args.num_layers
        self.n_vocab = len(dictionary)

        self.dictionary = dictionary
        self.padding_idx = dictionary.pad()
        self.cls_idx = dictionary.indices["<cls>"]
        self.q_idx = dictionary.indices["<q>"]

        self.embedding = nn.Embedding(self.n_vocab, self.n_embed)
        self.encoder = nn.LSTM(self.n_embed, self.n_hidden, self.n_layers, batch_first=True)

        self.adaptive_length = 8
        self.fc_logits = nn.Linear(self.adaptive_length * self.n_hidden, 4)
        ##############################################################################
        #                              END OF YOUR CODE                              #
        ##############################################################################

    def logits(self, **kwargs):
        """
        Compute the logits for the input data.

        Args:
            kwargs (dict): Custom keyword arguments containing input data.
                          Modify the input processing according to your needs.

        Returns:
            logits (tensor): Logits generated by your model given input.
        """
        # ##############################################################################
        #                  TODO: You need to complete the code here                  #
        ##############################################################################
        contents = kwargs["content"]
        # contents: [batch_size, max_content_len]
        questions = kwargs["q"]
        # questions: [batch_size, max_q_len]
        choices = kwargs["choices"]
        # choices: [batch_size, 4, max_choices_len]

        batch_size = contents.shape[0]

        contents_embed = self.embedding(contents)
        # contents_embed: [batch_size, max_content_len, n_embed]
        q_embed = self.embedding(torch.tensor(self.q_idx, device=next(self.parameters()).device)) # [n_embed]
        questions_embed = self.embedding(questions)
        # questions_embed: [batch_size, max_q_len, n_embed]
        cls_embed = self.embedding(torch.tensor(self.cls_idx, device=next(self.parameters()).device)) # [n_embed]
        choices_embed = self.embedding(choices)
        # choices_embed: [batch_size, 4, max_choices_len, n_embed]

        # inputs: [batch_size, max_content_len + <q> + max_q_len + (<cls> + max_choices_len) * 4, n_embed]
        # print("contents_embed shape: ", contents_embed.shape)
        # print("q_embed shape: ", q_embed.repeat(batch_size, 1, 1).shape)
        # print("questions_embed shape: ", questions_embed.shape)
        # print("cls_embed shape: ", cls_embed.repeat(batch_size, 1, 1).shape)
        # print("choices_embed[:, 0] shape: ", choices_embed[:, 0].shape)
        # print("choices_embed[:, 1] shape: ", choices_embed[:, 1].shape)
        inputs = torch.cat([
            contents_embed,
            q_embed.repeat(batch_size, 1, 1),
            questions_embed,
            cls_embed.repeat(batch_size, 1, 1),
            choices_embed[:, 0],
            cls_embed.repeat(batch_size, 1, 1),
            choices_embed[:, 1],
            cls_embed.repeat(batch_size, 1, 1),
            choices_embed[:, 2],
            cls_embed.repeat(batch_size, 1, 1),
            choices_embed[:, 3],
        ], dim=1)
        # print("inputs.shape: ", inputs.shape)

        outputs, (hidden_state, cell_state) = self.encoder(inputs)
        # outputs: [batch_size, max_content_len + <q> + max_q_len + (<cls> + max_choices_len) * 4, n_hidden]
        outputs_pooled = F.adaptive_avg_pool1d(outputs.transpose(1, 2), self.adaptive_length)
        # outputs_pooled: [batch_size, n_hidden * self.adaptive_length]
        # print("outputs_pooled.shape: ", outputs_pooled.shape)
        # print("batch_size: ", batch_size)
        outputs_pooled = outputs_pooled.view(batch_size, -1)
        # print("outputs_pooled.shape: ", outputs_pooled.shape)
        # print("self.fc_logits.weight.shape: ", self.fc_logits.weight.shape)
        logits = self.fc_logits(outputs_pooled)
        # logits: [batch_size, 4]

        invalid_choices_mask = (choices == self.padding_idx).all(dim=-1)
        # invalid_choices_mask: [batch_size, 4]

        logits.masked_fill_(invalid_choices_mask, -torch.inf)

        return logits
        ##############################################################################
        #                              END OF YOUR CODE                              #
        ##############################################################################
    
    def lstm_with_attention(
            self,
            encoder_outputs, # [batch_size, max_content_len, n_hidden]
            encoder_hidden, # ([n_layers, batch_size, n_hidden]) * 2
            decoder_input, # [batch_size, seq_len, n_embed]
    ):
        hidden_state, cell_state = encoder_hidden
        decoder_outputs = []
        for i in range(decoder_input.shape[1]):
            query = hidden_state[-1].unsqueeze(1)
            # query: [batch_size, 1, n_hidden]
            key = self.fc_attn(encoder_outputs)
            # key: [batch_size, max_content_len, n_hidden]
            attn_scores = torch.bmm(query, key.transpose(1, 2))
            attn_probs = F.softmax(attn_scores, dim=-1)
            # attn_probs: [batch_size, 1, max_content_len]
            context = torch.bmm(attn_probs, encoder_outputs)
            # context: [batch_size, 1, n_hidden]

            # print("context.shape: ", context.shape)
            # print("decoder_input[:, i, :].unsqueeze(1).shape: ", decoder_input[:, i, :].unsqueeze(1).shape)
            inputs = torch.cat([decoder_input[:, i, :].unsqueeze(1), context], dim=-1)
            # inputs: [batch_size, 1, n_embed + n_hidden]
            outputs, (hidden_state, cell_state) = self.decoder(inputs, (hidden_state, cell_state))

            decoder_outputs.append(outputs)
        decoder_outputs = torch.cat(decoder_outputs, dim=1)
        # decoder_outputs: [batch_size, seq_len, n_hidden]
        return decoder_outputs, (hidden_state, cell_state)

    def get_loss(self, **kwargs):
        """
        Compute the loss for the input data and target labels.

        Args:
            kwargs (dict): Custom keyword arguments containing input data and
                           target labels. Modify the input and target processing
                           according to your needs.

        Returns:
            loss (tensor): The loss for the input data and target labels.
        """
        # ##############################################################################
        #                  TODO: You need to complete the code here                  #
        ##############################################################################
        targets = kwargs.pop("targets")
        logits = self.logits(**kwargs)
        return F.cross_entropy(logits, targets)
        ##############################################################################
        #                              END OF YOUR CODE                              #
        ##############################################################################
