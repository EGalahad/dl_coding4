import copy
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from transformers import AutoModel, AutoTokenizer
class Net(nn.Module):

    def __init__(self, args, model_name="bert-base-chinese", max_len=5120):
        super().__init__()
        ##############################################################################
        #                  TODO: You need to complete the code here                  #
        ##############################################################################
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)

        # adapt the model to longer input
        self.model.embeddings.position_embeddings = nn.Embedding(max_len, self.model.embeddings.position_embeddings.embedding_dim)
        self.model.embeddings.register_buffer("position_ids", torch.arange(max_len, dtype=torch.long).unsqueeze(0))
        self.model.embeddings.register_buffer("token_type_ids", torch.zeros(1, max_len, dtype=torch.long))

        self.dropout = nn.Dropout(self.model.config.hidden_dropout_prob)

        self.fc_logits = nn.Linear(self.model.config.hidden_size, 1)
        ##############################################################################
        #                              END OF YOUR CODE                              #
        ##############################################################################

    def logits(self, **kwargs):
        """
        Compute the logits for the input data.

        Args:
            kwargs (dict): Custom keyword arguments containing input data.
                          Modify the input processing according to your needs.

        Returns:
            logits (tensor): Logits generated by your model given input.
        """
        # ##############################################################################
        #                  TODO: You need to complete the code here                  #
        ##############################################################################
        batch_size = kwargs["input_ids"].shape[0]
        num_choices = kwargs["input_ids"].shape[1]
        kwargs = {k: v.view(batch_size * num_choices, -1) for k, v in kwargs.items()}
        pooler_output = self.model(**kwargs)[1]
        pooler_output = self.dropout(pooler_output)
        # pooler_output: [batch_size * num_choices, hidden_size]
        logits = self.fc_logits(pooler_output).view(batch_size, num_choices)
        return logits
        ##############################################################################
        #                              END OF YOUR CODE                              #
        ##############################################################################
    
    def get_loss(self, **kwargs):
        """
        Compute the loss for the input data and target labels.

        Args:
            kwargs (dict): Custom keyword arguments containing input data and
                           target labels. Modify the input and target processing
                           according to your needs.

        Returns:
            loss (tensor): The loss for the input data and target labels.
        """
        # ##############################################################################
        #                  TODO: You need to complete the code here                  #
        ##############################################################################
        targets = kwargs.pop("targets")
        # targets: [batch_size]
        logits = self.logits(**kwargs)
        return F.cross_entropy(logits, targets)
        ##############################################################################
        #                              END OF YOUR CODE                              #
        ##############################################################################
